{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baed00ee-9461-4560-8813-31a571c86877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 05:22:51.215953: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import setigen as stg\n",
    "from blimpy import Waterfall\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from astropy import units as u\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import silhouette_score\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f020ec-5663-42ab-8527-2b5d37948ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e00c928-f039-4774-8477-10c9e940e2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def painting(data):\n",
    "    all_data = []\n",
    "    labels = []\n",
    "    for c in range(num_classes):\n",
    "        drift = 2*random.random()*(-1)**random.randint(0,2)\n",
    "        snr = random.randint(100, 150)\n",
    "        width = random.randint(20, 50)\n",
    "        for s in range(num_samples_per_class):\n",
    "            index = random.randint(0, data.shape[0]-1)\n",
    "            window = data[index, :,:]\n",
    "            \n",
    "            start = random.randint(50, 180)\n",
    "            \n",
    "            frame = stg.Frame.from_data(df=2.7939677238464355*u.Hz,\n",
    "                                        dt=18.253611008*u.s,\n",
    "                                        fch1=1289*u.MHz,\n",
    "                                        ascending=True,\n",
    "                                        data=window)\n",
    "            frame.add_signal(stg.constant_path(\n",
    "                                        f_start=frame.get_frequency(index=start),\n",
    "                                       drift_rate=drift*u.Hz/u.s),\n",
    "                                      stg.constant_t_profile(level=frame.get_intensity(snr=snr)),\n",
    "                                      stg.gaussian_f_profile(width=width*u.Hz),\n",
    "                                      stg.constant_bp_profile(level=1))\n",
    "            all_data.append(frame.data)\n",
    "            labels.append(c)\n",
    "    all_data = np.array(all_data)\n",
    "    labels = np.vstack(labels)\n",
    "    return all_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20e49e4f-15bd-4a32-a6d5-7a3274be3b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f54907-bc05-4c64-a362-5bcc5511de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = 8\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"kl_loss\"\n",
    "        )\n",
    "        self.kl_additional = tf.keras.losses.KLDivergence()\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "        ]\n",
    "    def gaussanity_loss(self, data, base):\n",
    "        return self.kl_additional(data, base)\n",
    "    \n",
    "    def train_step(self, data_in):\n",
    "        data = data_in\n",
    "        print(data.shape)\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            \n",
    "            \n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        \n",
    "        mse_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.mse(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(mse_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "        }\n",
    "    def test_step(self, data_in):\n",
    "        data, _ = data_in\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "            )\n",
    "        )\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss \n",
    "        \n",
    "        mse_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.mse(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(mse_loss)\n",
    "        return {\n",
    "            \"test_loss\": self.total_loss_tracker.result(),\n",
    "            \"test_kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"test_reconstruction_loss\": self.reconstruction_loss_tracker.result()\n",
    "        }\n",
    "    def __call__ (self, inputs):\n",
    "        return self.decoder(self.encoder(inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4d21f4-47eb-4ec1-a211-13e63440ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531b40be-4015-40db-926c-0e681083194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 05:22:53.329991: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-24 05:22:53.844609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13888 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:a1:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 16, 256, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 16, 256, 32)  320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 16, 128, 32)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 16, 128, 32)  128        ['max_pooling2d[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 16, 128, 64)  18496       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 16, 64, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 16, 64, 64)  256         ['max_pooling2d_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 16, 64, 64)   36928       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 16, 32, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16, 32, 64)  256         ['max_pooling2d_2[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 32, 128)  73856       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0          ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 16, 16, 128)  512        ['max_pooling2d_3[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 16, 16, 128)  147584      ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 16, 8, 128)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 8, 128)  512         ['max_pooling2d_4[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 16384)        0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          4194560     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 256)         1024        ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           8224        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32)          128         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           1056        ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           1056        ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32)          128         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32)          128         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 10)           330         ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 10)           330         ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 10)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,485,812\n",
      "Trainable params: 4,484,276\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 10\n",
    "time_samples = 16\n",
    "freq_sample =  256\n",
    "encoder_inputs = keras.Input(shape=(time_samples, freq_sample, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=1, padding=\"same\")(encoder_inputs)\n",
    "x = layers.MaxPool2D(pool_size=(1, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(1, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(1, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(128, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(1, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(128, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(1, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x_shape = x.shape\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "z_mean = layers.Dense(32, activation=\"relu\")(x)\n",
    "z_mean = layers.BatchNormalization()(z_mean)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(z_mean)\n",
    "\n",
    "z_log_var = layers.Dense(32, activation=\"relu\")(x)\n",
    "z_log_var = layers.BatchNormalization()(z_log_var)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(z_log_var)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7131cf6f-ba9a-43ea-8b45-d3f79bd5acd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               2816      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16384)             4210688   \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 16384)            65536     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 16, 8, 128)        0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 32, 16, 128)      147584    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 16, 16, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 128)      147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 16, 32, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 16, 32, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 32, 64, 64)       73792     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 16, 64, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 16, 64, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 32, 128, 64)      36928     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 16, 128, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 16, 128, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 32, 256, 32)      18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 16, 256, 32)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 16, 256, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 16, 256, 1)       289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,706,369\n",
      "Trainable params: 4,672,257\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(256, activation=\"relu\")(latent_inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(x_shape[1]* x_shape[2]* x_shape[3], activation=\"relu\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Reshape((x_shape[1], x_shape[2], x_shape[3]))(x)\n",
    "x = layers.Conv2DTranspose(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"linear\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0074643-eba7-4961-aa51-3916eb75c72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f110317bc10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = VAE(encoder, decoder)\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate = 1e-3))\n",
    "# autoencoder.load_weights(\"../b-vae/models/full-weights-\"+'07-02-2023-15-19-23')\n",
    "autoencoder.load_weights(\"../b-vae/models/full-weights-\"+'08-23-2023-21-05-49')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df2f403c-1e31-45be-9f8b-44277bc1bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    epsilon = 1\n",
    "    min_val = data.min()\n",
    "    data = data - min_val + epsilon\n",
    "    new_data = np.log(data)\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "    final_data = (data - min_val) / (max_val - min_val)\n",
    "    return final_data\n",
    "    \n",
    "def normalize_data(data):\n",
    "    for i in tqdm(range(data.shape[0])):\n",
    "        data[i,:,:] = normalize(data[i,:,:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "218b0964-8671-484c-b24f-e9081b651743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def idealized(num=10_000):\n",
    "    drift = 0\n",
    "    snr = 20\n",
    "    width = 10\n",
    "    start = 256//2\n",
    "    data = []\n",
    "    labels = []\n",
    "    for tag in range(4):\n",
    "        label_vec = np.zeros(4)\n",
    "        label_vec[tag] = 1\n",
    "        for i in range(num):\n",
    "            if tag == 0:\n",
    "                drift = (-1)**(random.randint(0,2)) * 2 *random.random()\n",
    "            elif tag == 1:\n",
    "                snr = 50*random.random() +20\n",
    "            elif tag == 2:\n",
    "                width = 50*random.random() +20\n",
    "            elif tag == 3:\n",
    "                start = random.randint(50, 180)\n",
    "\n",
    "            frame = stg.Frame(fchans=256*u.pixel,\n",
    "                              tchans=16*u.pixel,\n",
    "                              df=2.7939677238464355*u.Hz,\n",
    "                              dt=18.253611008*u.s,\n",
    "                              fch1=6095.214842353016*u.MHz)\n",
    "            noise = frame.add_noise(x_mean=1, noise_type='chi2')\n",
    "            frame.add_signal(stg.constant_path(\n",
    "                                        f_start=frame.get_frequency(index=start),\n",
    "                                       drift_rate=drift*u.Hz/u.s),\n",
    "                                      stg.constant_t_profile(level=frame.get_intensity(snr=snr)),\n",
    "                                      stg.gaussian_f_profile(width=width*u.Hz),\n",
    "                                      stg.constant_bp_profile(level=1))\n",
    "            data.append(frame.data)\n",
    "            labels.append(label_vec)\n",
    "    data = np.array(data)\n",
    "    labels = np.vstack(labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9549969-98f3-4658-a0f7-ff09b604ff4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                              | 0/10 [00:00<?, ?it/s]\n",
      "  0%|                                                                                           | 0/40000 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|██▎                                                                          | 1174/40000 [00:00<00:03, 11731.20it/s]\u001b[A\n",
      "  6%|████▌                                                                        | 2353/40000 [00:00<00:03, 11762.07it/s]\u001b[A\n",
      "  9%|██████▊                                                                      | 3538/40000 [00:00<00:03, 11798.23it/s]\u001b[A\n",
      " 12%|█████████                                                                    | 4718/40000 [00:00<00:02, 11782.27it/s]\u001b[A\n",
      " 15%|███████████▎                                                                 | 5900/40000 [00:00<00:02, 11793.54it/s]\u001b[A\n",
      " 18%|█████████████▋                                                               | 7081/40000 [00:00<00:02, 11798.83it/s]\u001b[A\n",
      " 21%|███████████████▉                                                             | 8261/40000 [00:00<00:02, 11486.24it/s]\u001b[A\n",
      " 24%|██████████████████▏                                                          | 9447/40000 [00:00<00:02, 11602.25it/s]\u001b[A\n",
      " 27%|████████████████████▏                                                       | 10610/40000 [00:00<00:02, 11608.12it/s]\u001b[A\n",
      " 29%|██████████████████████▎                                                     | 11772/40000 [00:01<00:02, 11513.64it/s]\u001b[A\n",
      " 32%|████████████████████████▌                                                   | 12947/40000 [00:01<00:02, 11584.24it/s]\u001b[A\n",
      " 35%|██████████████████████████▊                                                 | 14134/40000 [00:01<00:02, 11670.16it/s]\u001b[A\n",
      " 38%|█████████████████████████████                                               | 15302/40000 [00:01<00:02, 11592.73it/s]\u001b[A\n",
      " 41%|███████████████████████████████▎                                            | 16492/40000 [00:01<00:02, 11682.04it/s]\u001b[A\n",
      " 44%|█████████████████████████████████▌                                          | 17678/40000 [00:01<00:01, 11732.94it/s]\u001b[A\n",
      " 47%|███████████████████████████████████▊                                        | 18865/40000 [00:01<00:01, 11772.26it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████                                      | 20055/40000 [00:01<00:01, 11808.88it/s]\u001b[A\n",
      " 53%|████████████████████████████████████████▎                                   | 21243/40000 [00:01<00:01, 11829.43it/s]\u001b[A\n",
      " 56%|██████████████████████████████████████████▌                                 | 22432/40000 [00:01<00:01, 11846.13it/s]\u001b[A\n",
      " 59%|████████████████████████████████████████████▉                               | 23619/40000 [00:02<00:01, 11851.86it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████▏                            | 24805/40000 [00:02<00:01, 11839.85it/s]\u001b[A\n",
      " 65%|█████████████████████████████████████████████████▍                          | 25990/40000 [00:02<00:01, 11704.23it/s]\u001b[A\n",
      " 68%|███████████████████████████████████████████████████▋                        | 27174/40000 [00:02<00:01, 11742.83it/s]\u001b[A\n",
      " 71%|█████████████████████████████████████████████████████▉                      | 28358/40000 [00:02<00:00, 11771.11it/s]\u001b[A\n",
      " 74%|████████████████████████████████████████████████████████▏                   | 29540/40000 [00:02<00:00, 11784.98it/s]\u001b[A\n",
      " 77%|██████████████████████████████████████████████████████████▍                 | 30724/40000 [00:02<00:00, 11799.04it/s]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████████▌               | 31905/40000 [00:02<00:00, 11674.88it/s]\u001b[A\n",
      " 83%|██████████████████████████████████████████████████████████████▊             | 33091/40000 [00:02<00:00, 11728.10it/s]\u001b[A\n",
      " 86%|█████████████████████████████████████████████████████████████████▏          | 34277/40000 [00:02<00:00, 11764.44it/s]\u001b[A\n",
      " 89%|███████████████████████████████████████████████████████████████████▎        | 35454/40000 [00:03<00:00, 11656.03it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████████████████████████████████████▌      | 36640/40000 [00:03<00:00, 11715.67it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████████████████████████████████████▊    | 37812/40000 [00:03<00:00, 11617.87it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:03<00:00, 11706.79it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "(40000, 16, 256, 1)\n",
      "(40000, 10)\n",
      "Epoch 1/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 1.5597 - accuracy: 0.4440 - val_loss: 1.1978 - val_accuracy: 0.4675\n",
      "Epoch 2/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 1.0805 - accuracy: 0.5777 - val_loss: 0.9751 - val_accuracy: 0.6257\n",
      "Epoch 3/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.9179 - accuracy: 0.6345 - val_loss: 0.8514 - val_accuracy: 0.6888\n",
      "Epoch 4/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.8116 - accuracy: 0.7285 - val_loss: 0.7640 - val_accuracy: 0.7578\n",
      "Epoch 5/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.7340 - accuracy: 0.7978 - val_loss: 0.6975 - val_accuracy: 0.8045\n",
      "Epoch 6/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6748 - accuracy: 0.8196 - val_loss: 0.6451 - val_accuracy: 0.8295\n",
      "Epoch 7/150\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.8372 - val_loss: 0.6040 - val_accuracy: 0.8384\n",
      "Epoch 8/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.5888 - accuracy: 0.8445 - val_loss: 0.5684 - val_accuracy: 0.8489\n",
      "Epoch 9/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.5560 - accuracy: 0.8483 - val_loss: 0.5378 - val_accuracy: 0.8578\n",
      "Epoch 10/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.5278 - accuracy: 0.8527 - val_loss: 0.5119 - val_accuracy: 0.8608\n",
      "Epoch 11/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.5032 - accuracy: 0.8560 - val_loss: 0.4896 - val_accuracy: 0.8575\n",
      "Epoch 12/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.4818 - accuracy: 0.8579 - val_loss: 0.4699 - val_accuracy: 0.8563\n",
      "Epoch 13/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.4628 - accuracy: 0.8591 - val_loss: 0.4514 - val_accuracy: 0.8634\n",
      "Epoch 14/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.4457 - accuracy: 0.8609 - val_loss: 0.4350 - val_accuracy: 0.8668\n",
      "Epoch 15/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.4306 - accuracy: 0.8621 - val_loss: 0.4204 - val_accuracy: 0.8694\n",
      "Epoch 16/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.4167 - accuracy: 0.8648 - val_loss: 0.4078 - val_accuracy: 0.8705\n",
      "Epoch 17/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.4043 - accuracy: 0.8666 - val_loss: 0.3962 - val_accuracy: 0.8705\n",
      "Epoch 18/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3930 - accuracy: 0.8657 - val_loss: 0.3852 - val_accuracy: 0.8757\n",
      "Epoch 19/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.3827 - accuracy: 0.8677 - val_loss: 0.3754 - val_accuracy: 0.8802\n",
      "Epoch 20/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.3734 - accuracy: 0.8708 - val_loss: 0.3675 - val_accuracy: 0.8769\n",
      "Epoch 21/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3648 - accuracy: 0.8698 - val_loss: 0.3584 - val_accuracy: 0.8813\n",
      "Epoch 22/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.3570 - accuracy: 0.8711 - val_loss: 0.3512 - val_accuracy: 0.8806\n",
      "Epoch 23/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.3498 - accuracy: 0.8726 - val_loss: 0.3441 - val_accuracy: 0.8854\n",
      "Epoch 24/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.3430 - accuracy: 0.8727 - val_loss: 0.3384 - val_accuracy: 0.8784\n",
      "Epoch 25/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.3367 - accuracy: 0.8730 - val_loss: 0.3316 - val_accuracy: 0.8862\n",
      "Epoch 26/150\n",
      "189/189 [==============================] - 3s 15ms/step - loss: 0.3312 - accuracy: 0.8744 - val_loss: 0.3262 - val_accuracy: 0.8851\n",
      "Epoch 27/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.3259 - accuracy: 0.8748 - val_loss: 0.3212 - val_accuracy: 0.8854\n",
      "Epoch 28/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.3210 - accuracy: 0.8747 - val_loss: 0.3171 - val_accuracy: 0.8806\n",
      "Epoch 29/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.3165 - accuracy: 0.8755 - val_loss: 0.3122 - val_accuracy: 0.8847\n",
      "Epoch 30/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.3122 - accuracy: 0.8759 - val_loss: 0.3088 - val_accuracy: 0.8828\n",
      "Epoch 31/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.3084 - accuracy: 0.8768 - val_loss: 0.3045 - val_accuracy: 0.8825\n",
      "Epoch 32/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.3047 - accuracy: 0.8765 - val_loss: 0.3009 - val_accuracy: 0.8828\n",
      "Epoch 33/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.3013 - accuracy: 0.8779 - val_loss: 0.2975 - val_accuracy: 0.8840\n",
      "Epoch 34/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2981 - accuracy: 0.8775 - val_loss: 0.2945 - val_accuracy: 0.8847\n",
      "Epoch 35/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2951 - accuracy: 0.8789 - val_loss: 0.2929 - val_accuracy: 0.8765\n",
      "Epoch 36/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2924 - accuracy: 0.8780 - val_loss: 0.2890 - val_accuracy: 0.8828\n",
      "Epoch 37/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2898 - accuracy: 0.8787 - val_loss: 0.2868 - val_accuracy: 0.8836\n",
      "Epoch 38/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2873 - accuracy: 0.8790 - val_loss: 0.2838 - val_accuracy: 0.8858\n",
      "Epoch 39/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2851 - accuracy: 0.8793 - val_loss: 0.2820 - val_accuracy: 0.8806\n",
      "Epoch 40/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2830 - accuracy: 0.8799 - val_loss: 0.2802 - val_accuracy: 0.8825\n",
      "Epoch 41/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2809 - accuracy: 0.8794 - val_loss: 0.2776 - val_accuracy: 0.8847\n",
      "Epoch 42/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2790 - accuracy: 0.8795 - val_loss: 0.2764 - val_accuracy: 0.8799\n",
      "Epoch 43/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.2772 - accuracy: 0.8801 - val_loss: 0.2750 - val_accuracy: 0.8784\n",
      "Epoch 44/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2755 - accuracy: 0.8802 - val_loss: 0.2723 - val_accuracy: 0.8836\n",
      "Epoch 45/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.2739 - accuracy: 0.8797 - val_loss: 0.2710 - val_accuracy: 0.8840\n",
      "Epoch 46/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.2724 - accuracy: 0.8807 - val_loss: 0.2698 - val_accuracy: 0.8806\n",
      "Epoch 47/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2710 - accuracy: 0.8810 - val_loss: 0.2685 - val_accuracy: 0.8836\n",
      "Epoch 48/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2696 - accuracy: 0.8812 - val_loss: 0.2665 - val_accuracy: 0.8843\n",
      "Epoch 49/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2682 - accuracy: 0.8814 - val_loss: 0.2664 - val_accuracy: 0.8787\n",
      "Epoch 50/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2671 - accuracy: 0.8811 - val_loss: 0.2643 - val_accuracy: 0.8847\n",
      "Epoch 51/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2659 - accuracy: 0.8813 - val_loss: 0.2629 - val_accuracy: 0.8843\n",
      "Epoch 52/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2647 - accuracy: 0.8820 - val_loss: 0.2624 - val_accuracy: 0.8825\n",
      "Epoch 53/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2636 - accuracy: 0.8825 - val_loss: 0.2621 - val_accuracy: 0.8787\n",
      "Epoch 54/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2627 - accuracy: 0.8818 - val_loss: 0.2601 - val_accuracy: 0.8810\n",
      "Epoch 55/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2615 - accuracy: 0.8827 - val_loss: 0.2607 - val_accuracy: 0.8780\n",
      "Epoch 56/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2608 - accuracy: 0.8824 - val_loss: 0.2591 - val_accuracy: 0.8799\n",
      "Epoch 57/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2598 - accuracy: 0.8827 - val_loss: 0.2576 - val_accuracy: 0.8821\n",
      "Epoch 58/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2589 - accuracy: 0.8831 - val_loss: 0.2560 - val_accuracy: 0.8869\n",
      "Epoch 59/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2580 - accuracy: 0.8835 - val_loss: 0.2554 - val_accuracy: 0.8862\n",
      "Epoch 60/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2572 - accuracy: 0.8838 - val_loss: 0.2551 - val_accuracy: 0.8832\n",
      "Epoch 61/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2564 - accuracy: 0.8835 - val_loss: 0.2543 - val_accuracy: 0.8828\n",
      "Epoch 62/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2557 - accuracy: 0.8841 - val_loss: 0.2529 - val_accuracy: 0.8888\n",
      "Epoch 63/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2548 - accuracy: 0.8845 - val_loss: 0.2533 - val_accuracy: 0.8825\n",
      "Epoch 64/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2541 - accuracy: 0.8847 - val_loss: 0.2516 - val_accuracy: 0.8873\n",
      "Epoch 65/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2535 - accuracy: 0.8848 - val_loss: 0.2513 - val_accuracy: 0.8854\n",
      "Epoch 66/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2527 - accuracy: 0.8844 - val_loss: 0.2500 - val_accuracy: 0.8896\n",
      "Epoch 67/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2520 - accuracy: 0.8849 - val_loss: 0.2495 - val_accuracy: 0.8858\n",
      "Epoch 68/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2513 - accuracy: 0.8857 - val_loss: 0.2485 - val_accuracy: 0.8903\n",
      "Epoch 69/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.2508 - accuracy: 0.8852 - val_loss: 0.2484 - val_accuracy: 0.8862\n",
      "Epoch 70/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2501 - accuracy: 0.8856 - val_loss: 0.2479 - val_accuracy: 0.8858\n",
      "Epoch 71/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2495 - accuracy: 0.8854 - val_loss: 0.2469 - val_accuracy: 0.8896\n",
      "Epoch 72/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2490 - accuracy: 0.8860 - val_loss: 0.2463 - val_accuracy: 0.8877\n",
      "Epoch 73/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2484 - accuracy: 0.8860 - val_loss: 0.2460 - val_accuracy: 0.8869\n",
      "Epoch 74/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2478 - accuracy: 0.8863 - val_loss: 0.2455 - val_accuracy: 0.8866\n",
      "Epoch 75/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.2473 - accuracy: 0.8867 - val_loss: 0.2449 - val_accuracy: 0.8873\n",
      "Epoch 76/150\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 0.2467 - accuracy: 0.8864 - val_loss: 0.2441 - val_accuracy: 0.8910\n",
      "Epoch 77/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2462 - accuracy: 0.8862 - val_loss: 0.2443 - val_accuracy: 0.8854\n",
      "Epoch 78/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2458 - accuracy: 0.8866 - val_loss: 0.2437 - val_accuracy: 0.8862\n",
      "Epoch 79/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2453 - accuracy: 0.8871 - val_loss: 0.2440 - val_accuracy: 0.8851\n",
      "Epoch 80/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2448 - accuracy: 0.8869 - val_loss: 0.2426 - val_accuracy: 0.8869\n",
      "Epoch 81/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2442 - accuracy: 0.8875 - val_loss: 0.2434 - val_accuracy: 0.8851\n",
      "Epoch 82/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2438 - accuracy: 0.8878 - val_loss: 0.2416 - val_accuracy: 0.8873\n",
      "Epoch 83/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2433 - accuracy: 0.8882 - val_loss: 0.2414 - val_accuracy: 0.8869\n",
      "Epoch 84/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2428 - accuracy: 0.8879 - val_loss: 0.2404 - val_accuracy: 0.8896\n",
      "Epoch 85/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2424 - accuracy: 0.8882 - val_loss: 0.2406 - val_accuracy: 0.8877\n",
      "Epoch 86/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2420 - accuracy: 0.8885 - val_loss: 0.2400 - val_accuracy: 0.8869\n",
      "Epoch 87/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2415 - accuracy: 0.8883 - val_loss: 0.2390 - val_accuracy: 0.8907\n",
      "Epoch 88/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2411 - accuracy: 0.8891 - val_loss: 0.2392 - val_accuracy: 0.8877\n",
      "Epoch 89/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2407 - accuracy: 0.8891 - val_loss: 0.2387 - val_accuracy: 0.8877\n",
      "Epoch 90/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.2403 - accuracy: 0.8892 - val_loss: 0.2380 - val_accuracy: 0.8881\n",
      "Epoch 91/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2398 - accuracy: 0.8895 - val_loss: 0.2379 - val_accuracy: 0.8884\n",
      "Epoch 92/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2395 - accuracy: 0.8898 - val_loss: 0.2371 - val_accuracy: 0.8903\n",
      "Epoch 93/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2390 - accuracy: 0.8898 - val_loss: 0.2371 - val_accuracy: 0.8884\n",
      "Epoch 94/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2387 - accuracy: 0.8904 - val_loss: 0.2362 - val_accuracy: 0.8903\n",
      "Epoch 95/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.2383 - accuracy: 0.8903 - val_loss: 0.2362 - val_accuracy: 0.8888\n",
      "Epoch 96/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2379 - accuracy: 0.8902 - val_loss: 0.2357 - val_accuracy: 0.8888\n",
      "Epoch 97/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2376 - accuracy: 0.8911 - val_loss: 0.2355 - val_accuracy: 0.8892\n",
      "Epoch 98/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.2372 - accuracy: 0.8907 - val_loss: 0.2354 - val_accuracy: 0.8881\n",
      "Epoch 99/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2368 - accuracy: 0.8910 - val_loss: 0.2348 - val_accuracy: 0.8896\n",
      "Epoch 100/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2365 - accuracy: 0.8917 - val_loss: 0.2345 - val_accuracy: 0.8888\n",
      "Epoch 101/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2361 - accuracy: 0.8918 - val_loss: 0.2342 - val_accuracy: 0.8881\n",
      "Epoch 102/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2358 - accuracy: 0.8919 - val_loss: 0.2341 - val_accuracy: 0.8877\n",
      "Epoch 103/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2355 - accuracy: 0.8919 - val_loss: 0.2338 - val_accuracy: 0.8881\n",
      "Epoch 104/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2351 - accuracy: 0.8922 - val_loss: 0.2337 - val_accuracy: 0.8884\n",
      "Epoch 105/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2349 - accuracy: 0.8926 - val_loss: 0.2330 - val_accuracy: 0.8881\n",
      "Epoch 106/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2345 - accuracy: 0.8928 - val_loss: 0.2325 - val_accuracy: 0.8888\n",
      "Epoch 107/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2342 - accuracy: 0.8929 - val_loss: 0.2325 - val_accuracy: 0.8892\n",
      "Epoch 108/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2339 - accuracy: 0.8929 - val_loss: 0.2316 - val_accuracy: 0.8907\n",
      "Epoch 109/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2336 - accuracy: 0.8923 - val_loss: 0.2318 - val_accuracy: 0.8888\n",
      "Epoch 110/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2333 - accuracy: 0.8931 - val_loss: 0.2315 - val_accuracy: 0.8873\n",
      "Epoch 111/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2329 - accuracy: 0.8939 - val_loss: 0.2310 - val_accuracy: 0.8896\n",
      "Epoch 112/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2326 - accuracy: 0.8934 - val_loss: 0.2306 - val_accuracy: 0.8896\n",
      "Epoch 113/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2324 - accuracy: 0.8942 - val_loss: 0.2304 - val_accuracy: 0.8888\n",
      "Epoch 114/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2321 - accuracy: 0.8939 - val_loss: 0.2305 - val_accuracy: 0.8903\n",
      "Epoch 115/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2318 - accuracy: 0.8942 - val_loss: 0.2303 - val_accuracy: 0.8903\n",
      "Epoch 116/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2315 - accuracy: 0.8947 - val_loss: 0.2297 - val_accuracy: 0.8896\n",
      "Epoch 117/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2313 - accuracy: 0.8941 - val_loss: 0.2291 - val_accuracy: 0.8903\n",
      "Epoch 118/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2309 - accuracy: 0.8949 - val_loss: 0.2288 - val_accuracy: 0.8914\n",
      "Epoch 119/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2306 - accuracy: 0.8947 - val_loss: 0.2302 - val_accuracy: 0.8933\n",
      "Epoch 120/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2305 - accuracy: 0.8939 - val_loss: 0.2283 - val_accuracy: 0.8925\n",
      "Epoch 121/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2301 - accuracy: 0.8953 - val_loss: 0.2289 - val_accuracy: 0.8907\n",
      "Epoch 122/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2299 - accuracy: 0.8951 - val_loss: 0.2283 - val_accuracy: 0.8884\n",
      "Epoch 123/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2296 - accuracy: 0.8959 - val_loss: 0.2296 - val_accuracy: 0.8933\n",
      "Epoch 124/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2295 - accuracy: 0.8946 - val_loss: 0.2276 - val_accuracy: 0.8910\n",
      "Epoch 125/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2292 - accuracy: 0.8955 - val_loss: 0.2277 - val_accuracy: 0.8925\n",
      "Epoch 126/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2290 - accuracy: 0.8957 - val_loss: 0.2271 - val_accuracy: 0.8918\n",
      "Epoch 127/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2287 - accuracy: 0.8960 - val_loss: 0.2265 - val_accuracy: 0.8933\n",
      "Epoch 128/150\n",
      "189/189 [==============================] - 3s 15ms/step - loss: 0.2284 - accuracy: 0.8957 - val_loss: 0.2270 - val_accuracy: 0.8922\n",
      "Epoch 129/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2282 - accuracy: 0.8959 - val_loss: 0.2261 - val_accuracy: 0.8940\n",
      "Epoch 130/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.2280 - accuracy: 0.8962 - val_loss: 0.2261 - val_accuracy: 0.8937\n",
      "Epoch 131/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2276 - accuracy: 0.8961 - val_loss: 0.2257 - val_accuracy: 0.8925\n",
      "Epoch 132/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2275 - accuracy: 0.8965 - val_loss: 0.2259 - val_accuracy: 0.8937\n",
      "Epoch 133/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2273 - accuracy: 0.8963 - val_loss: 0.2255 - val_accuracy: 0.8940\n",
      "Epoch 134/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2270 - accuracy: 0.8969 - val_loss: 0.2253 - val_accuracy: 0.8933\n",
      "Epoch 135/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2268 - accuracy: 0.8966 - val_loss: 0.2249 - val_accuracy: 0.8951\n",
      "Epoch 136/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2267 - accuracy: 0.8968 - val_loss: 0.2250 - val_accuracy: 0.8933\n",
      "Epoch 137/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.2263 - accuracy: 0.8976 - val_loss: 0.2243 - val_accuracy: 0.8981\n",
      "Epoch 138/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2262 - accuracy: 0.8970 - val_loss: 0.2251 - val_accuracy: 0.8929\n",
      "Epoch 139/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2260 - accuracy: 0.8976 - val_loss: 0.2242 - val_accuracy: 0.8940\n",
      "Epoch 140/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2257 - accuracy: 0.8973 - val_loss: 0.2241 - val_accuracy: 0.8929\n",
      "Epoch 141/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.2255 - accuracy: 0.8976 - val_loss: 0.2238 - val_accuracy: 0.8933\n",
      "Epoch 142/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2253 - accuracy: 0.8973 - val_loss: 0.2238 - val_accuracy: 0.8955\n",
      "Epoch 143/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2252 - accuracy: 0.8971 - val_loss: 0.2235 - val_accuracy: 0.8925\n",
      "Epoch 144/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2249 - accuracy: 0.8975 - val_loss: 0.2231 - val_accuracy: 0.8970\n",
      "Epoch 145/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2247 - accuracy: 0.8973 - val_loss: 0.2229 - val_accuracy: 0.8966\n",
      "Epoch 146/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2245 - accuracy: 0.8976 - val_loss: 0.2225 - val_accuracy: 0.9007\n",
      "Epoch 147/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2244 - accuracy: 0.8981 - val_loss: 0.2227 - val_accuracy: 0.8940\n",
      "Epoch 148/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2242 - accuracy: 0.8978 - val_loss: 0.2233 - val_accuracy: 0.8925\n",
      "Epoch 149/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2239 - accuracy: 0.8974 - val_loss: 0.2221 - val_accuracy: 0.8974\n",
      "Epoch 150/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2238 - accuracy: 0.8977 - val_loss: 0.2224 - val_accuracy: 0.8933\n",
      "413/413 [==============================] - 4s 10ms/step - loss: 0.2230 - accuracy: 0.8977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▌                                                                            | 1/10 [05:03<45:35, 304.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22301846742630005, 0.8977272510528564]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/40000 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|██▎                                                                          | 1196/40000 [00:00<00:03, 11954.34it/s]\u001b[A\n",
      "  6%|████▌                                                                        | 2392/40000 [00:00<00:03, 11874.51it/s]\u001b[A\n",
      "  9%|██████▉                                                                      | 3580/40000 [00:00<00:03, 11856.88it/s]\u001b[A\n",
      " 12%|█████████▏                                                                   | 4766/40000 [00:00<00:02, 11856.71it/s]\u001b[A\n",
      " 15%|███████████▍                                                                 | 5956/40000 [00:00<00:02, 11871.13it/s]\u001b[A\n",
      " 18%|█████████████▊                                                               | 7144/40000 [00:00<00:02, 11812.44it/s]\u001b[A\n",
      " 21%|████████████████                                                             | 8332/40000 [00:00<00:02, 11832.38it/s]\u001b[A\n",
      " 24%|██████████████████▎                                                          | 9522/40000 [00:00<00:02, 11852.98it/s]\u001b[A\n",
      " 27%|████████████████████▎                                                       | 10709/40000 [00:00<00:02, 11856.47it/s]\u001b[A\n",
      " 30%|██████████████████████▌                                                     | 11896/40000 [00:01<00:02, 11859.13it/s]\u001b[A\n",
      " 33%|████████████████████████▊                                                   | 13084/40000 [00:01<00:02, 11863.95it/s]\u001b[A\n",
      " 36%|███████████████████████████                                                 | 14272/40000 [00:01<00:02, 11867.30it/s]\u001b[A\n",
      " 39%|█████████████████████████████▎                                              | 15459/40000 [00:01<00:02, 11865.68it/s]\u001b[A\n",
      " 42%|███████████████████████████████▋                                            | 16646/40000 [00:01<00:02, 11420.06it/s]\u001b[A\n",
      " 44%|█████████████████████████████████▊                                          | 17792/40000 [00:01<00:01, 11427.43it/s]\u001b[A\n",
      " 47%|████████████████████████████████████                                        | 18976/40000 [00:01<00:01, 11547.55it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████▎                                     | 20160/40000 [00:01<00:01, 11633.69it/s]\u001b[A\n",
      " 53%|████████████████████████████████████████▌                                   | 21340/40000 [00:01<00:01, 11682.15it/s]\u001b[A\n",
      " 56%|██████████████████████████████████████████▊                                 | 22524/40000 [00:01<00:01, 11728.56it/s]\u001b[A\n",
      " 59%|█████████████████████████████████████████████                               | 23704/40000 [00:02<00:01, 11749.51it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████▎                            | 24882/40000 [00:02<00:01, 11756.47it/s]\u001b[A\n",
      " 65%|█████████████████████████████████████████████████▌                          | 26067/40000 [00:02<00:01, 11783.89it/s]\u001b[A\n",
      " 68%|███████████████████████████████████████████████████▊                        | 27250/40000 [00:02<00:01, 11796.99it/s]\u001b[A\n",
      " 71%|██████████████████████████████████████████████████████                      | 28433/40000 [00:02<00:00, 11804.98it/s]\u001b[A\n",
      " 74%|████████████████████████████████████████████████████████▎                   | 29616/40000 [00:02<00:00, 11811.51it/s]\u001b[A\n",
      " 77%|██████████████████████████████████████████████████████████▌                 | 30798/40000 [00:02<00:00, 11576.30it/s]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████████▊               | 31978/40000 [00:02<00:00, 11639.94it/s]\u001b[A\n",
      " 83%|██████████████████████████████████████████████████████████████▉             | 33143/40000 [00:02<00:00, 11503.77it/s]\u001b[A\n",
      " 86%|█████████████████████████████████████████████████████████████████▏          | 34295/40000 [00:02<00:00, 11502.08it/s]\u001b[A\n",
      " 89%|███████████████████████████████████████████████████████████████████▍        | 35477/40000 [00:03<00:00, 11596.14it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████████████████████████████████████▋      | 36661/40000 [00:03<00:00, 11665.95it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████████████████████████████████████▉    | 37846/40000 [00:03<00:00, 11720.20it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:03<00:00, 11727.42it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "(40000, 16, 256, 1)\n",
      "(40000, 10)\n",
      "Epoch 1/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 1.7885 - accuracy: 0.3206 - val_loss: 1.5102 - val_accuracy: 0.4078\n",
      "Epoch 2/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 1.3559 - accuracy: 0.4521 - val_loss: 1.1970 - val_accuracy: 0.5451\n",
      "Epoch 3/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 1.1071 - accuracy: 0.6367 - val_loss: 0.9954 - val_accuracy: 0.7701\n",
      "Epoch 4/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.9411 - accuracy: 0.7872 - val_loss: 0.8556 - val_accuracy: 0.8310\n",
      "Epoch 5/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.8223 - accuracy: 0.8179 - val_loss: 0.7527 - val_accuracy: 0.8485\n",
      "Epoch 6/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.7329 - accuracy: 0.8399 - val_loss: 0.6752 - val_accuracy: 0.8634\n",
      "Epoch 7/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.6631 - accuracy: 0.8548 - val_loss: 0.6129 - val_accuracy: 0.8709\n",
      "Epoch 8/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.6073 - accuracy: 0.8640 - val_loss: 0.5635 - val_accuracy: 0.8731\n",
      "Epoch 9/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.5618 - accuracy: 0.8712 - val_loss: 0.5224 - val_accuracy: 0.8832\n",
      "Epoch 10/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.5240 - accuracy: 0.8763 - val_loss: 0.4892 - val_accuracy: 0.8914\n",
      "Epoch 11/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.4921 - accuracy: 0.8820 - val_loss: 0.4599 - val_accuracy: 0.8963\n",
      "Epoch 12/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.4649 - accuracy: 0.8860 - val_loss: 0.4350 - val_accuracy: 0.8981\n",
      "Epoch 13/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.4414 - accuracy: 0.8894 - val_loss: 0.4138 - val_accuracy: 0.9030\n",
      "Epoch 14/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.4210 - accuracy: 0.8921 - val_loss: 0.3954 - val_accuracy: 0.9052\n",
      "Epoch 15/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.4031 - accuracy: 0.8947 - val_loss: 0.3790 - val_accuracy: 0.9090\n",
      "Epoch 16/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.3873 - accuracy: 0.8966 - val_loss: 0.3643 - val_accuracy: 0.9101\n",
      "Epoch 17/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.3733 - accuracy: 0.8990 - val_loss: 0.3515 - val_accuracy: 0.9093\n",
      "Epoch 18/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.3607 - accuracy: 0.9009 - val_loss: 0.3397 - val_accuracy: 0.9104\n",
      "Epoch 19/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.3493 - accuracy: 0.9017 - val_loss: 0.3297 - val_accuracy: 0.9090\n",
      "Epoch 20/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.3391 - accuracy: 0.9032 - val_loss: 0.3200 - val_accuracy: 0.9101\n",
      "Epoch 21/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.3300 - accuracy: 0.9039 - val_loss: 0.3117 - val_accuracy: 0.9104\n",
      "Epoch 22/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.3215 - accuracy: 0.9055 - val_loss: 0.3044 - val_accuracy: 0.9116\n",
      "Epoch 23/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.3138 - accuracy: 0.9065 - val_loss: 0.2974 - val_accuracy: 0.9123\n",
      "Epoch 24/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.3068 - accuracy: 0.9070 - val_loss: 0.2904 - val_accuracy: 0.9138\n",
      "Epoch 25/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.3004 - accuracy: 0.9078 - val_loss: 0.2847 - val_accuracy: 0.9149\n",
      "Epoch 26/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2945 - accuracy: 0.9084 - val_loss: 0.2794 - val_accuracy: 0.9146\n",
      "Epoch 27/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.2891 - accuracy: 0.9092 - val_loss: 0.2744 - val_accuracy: 0.9149\n",
      "Epoch 28/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2841 - accuracy: 0.9107 - val_loss: 0.2702 - val_accuracy: 0.9160\n",
      "Epoch 29/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2794 - accuracy: 0.9106 - val_loss: 0.2659 - val_accuracy: 0.9157\n",
      "Epoch 30/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2752 - accuracy: 0.9109 - val_loss: 0.2617 - val_accuracy: 0.9160\n",
      "Epoch 31/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2712 - accuracy: 0.9112 - val_loss: 0.2582 - val_accuracy: 0.9160\n",
      "Epoch 32/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2675 - accuracy: 0.9125 - val_loss: 0.2555 - val_accuracy: 0.9179\n",
      "Epoch 33/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2642 - accuracy: 0.9133 - val_loss: 0.2515 - val_accuracy: 0.9175\n",
      "Epoch 34/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2609 - accuracy: 0.9130 - val_loss: 0.2491 - val_accuracy: 0.9213\n",
      "Epoch 35/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2580 - accuracy: 0.9141 - val_loss: 0.2461 - val_accuracy: 0.9201\n",
      "Epoch 36/150\n",
      "189/189 [==============================] - 3s 17ms/step - loss: 0.2552 - accuracy: 0.9142 - val_loss: 0.2439 - val_accuracy: 0.9201\n",
      "Epoch 37/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2525 - accuracy: 0.9150 - val_loss: 0.2410 - val_accuracy: 0.9220\n",
      "Epoch 38/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2501 - accuracy: 0.9158 - val_loss: 0.2394 - val_accuracy: 0.9213\n",
      "Epoch 39/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2478 - accuracy: 0.9157 - val_loss: 0.2371 - val_accuracy: 0.9216\n",
      "Epoch 40/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2456 - accuracy: 0.9160 - val_loss: 0.2349 - val_accuracy: 0.9220\n",
      "Epoch 41/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2435 - accuracy: 0.9168 - val_loss: 0.2332 - val_accuracy: 0.9224\n",
      "Epoch 42/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2416 - accuracy: 0.9169 - val_loss: 0.2320 - val_accuracy: 0.9198\n",
      "Epoch 43/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2398 - accuracy: 0.9173 - val_loss: 0.2299 - val_accuracy: 0.9220\n",
      "Epoch 44/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2381 - accuracy: 0.9177 - val_loss: 0.2282 - val_accuracy: 0.9228\n",
      "Epoch 45/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2366 - accuracy: 0.9180 - val_loss: 0.2269 - val_accuracy: 0.9235\n",
      "Epoch 46/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2350 - accuracy: 0.9187 - val_loss: 0.2255 - val_accuracy: 0.9231\n",
      "Epoch 47/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2337 - accuracy: 0.9184 - val_loss: 0.2247 - val_accuracy: 0.9216\n",
      "Epoch 48/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2322 - accuracy: 0.9193 - val_loss: 0.2237 - val_accuracy: 0.9235\n",
      "Epoch 49/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2310 - accuracy: 0.9193 - val_loss: 0.2222 - val_accuracy: 0.9231\n",
      "Epoch 50/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.2298 - accuracy: 0.9196 - val_loss: 0.2209 - val_accuracy: 0.9246\n",
      "Epoch 51/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2286 - accuracy: 0.9199 - val_loss: 0.2204 - val_accuracy: 0.9243\n",
      "Epoch 52/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2274 - accuracy: 0.9203 - val_loss: 0.2194 - val_accuracy: 0.9228\n",
      "Epoch 53/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.2264 - accuracy: 0.9207 - val_loss: 0.2181 - val_accuracy: 0.9250\n",
      "Epoch 54/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2254 - accuracy: 0.9206 - val_loss: 0.2171 - val_accuracy: 0.9257\n",
      "Epoch 55/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2243 - accuracy: 0.9213 - val_loss: 0.2167 - val_accuracy: 0.9246\n",
      "Epoch 56/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.2235 - accuracy: 0.9215 - val_loss: 0.2157 - val_accuracy: 0.9272\n",
      "Epoch 57/150\n",
      "189/189 [==============================] - 4s 19ms/step - loss: 0.2226 - accuracy: 0.9217 - val_loss: 0.2146 - val_accuracy: 0.9265\n",
      "Epoch 58/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.2217 - accuracy: 0.9223 - val_loss: 0.2138 - val_accuracy: 0.9265\n",
      "Epoch 59/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2209 - accuracy: 0.9224 - val_loss: 0.2136 - val_accuracy: 0.9265\n",
      "Epoch 60/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2201 - accuracy: 0.9225 - val_loss: 0.2129 - val_accuracy: 0.9269\n",
      "Epoch 61/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2194 - accuracy: 0.9226 - val_loss: 0.2117 - val_accuracy: 0.9276\n",
      "Epoch 62/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2187 - accuracy: 0.9228 - val_loss: 0.2107 - val_accuracy: 0.9272\n",
      "Epoch 63/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2180 - accuracy: 0.9237 - val_loss: 0.2106 - val_accuracy: 0.9276\n",
      "Epoch 64/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2172 - accuracy: 0.9237 - val_loss: 0.2100 - val_accuracy: 0.9280\n",
      "Epoch 65/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2166 - accuracy: 0.9242 - val_loss: 0.2097 - val_accuracy: 0.9280\n",
      "Epoch 66/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2159 - accuracy: 0.9244 - val_loss: 0.2092 - val_accuracy: 0.9291\n",
      "Epoch 67/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2153 - accuracy: 0.9243 - val_loss: 0.2081 - val_accuracy: 0.9287\n",
      "Epoch 68/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2147 - accuracy: 0.9248 - val_loss: 0.2077 - val_accuracy: 0.9291\n",
      "Epoch 69/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2141 - accuracy: 0.9247 - val_loss: 0.2076 - val_accuracy: 0.9291\n",
      "Epoch 70/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2136 - accuracy: 0.9250 - val_loss: 0.2066 - val_accuracy: 0.9295\n",
      "Epoch 71/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2131 - accuracy: 0.9248 - val_loss: 0.2063 - val_accuracy: 0.9299\n",
      "Epoch 72/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2126 - accuracy: 0.9252 - val_loss: 0.2056 - val_accuracy: 0.9299\n",
      "Epoch 73/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2120 - accuracy: 0.9252 - val_loss: 0.2056 - val_accuracy: 0.9299\n",
      "Epoch 74/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2115 - accuracy: 0.9255 - val_loss: 0.2056 - val_accuracy: 0.9299\n",
      "Epoch 75/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2111 - accuracy: 0.9255 - val_loss: 0.2047 - val_accuracy: 0.9295\n",
      "Epoch 76/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.2106 - accuracy: 0.9259 - val_loss: 0.2044 - val_accuracy: 0.9291\n",
      "Epoch 77/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2102 - accuracy: 0.9259 - val_loss: 0.2038 - val_accuracy: 0.9306\n",
      "Epoch 78/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2097 - accuracy: 0.9259 - val_loss: 0.2033 - val_accuracy: 0.9310\n",
      "Epoch 79/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2093 - accuracy: 0.9259 - val_loss: 0.2026 - val_accuracy: 0.9306\n",
      "Epoch 80/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2089 - accuracy: 0.9263 - val_loss: 0.2029 - val_accuracy: 0.9302\n",
      "Epoch 81/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2085 - accuracy: 0.9264 - val_loss: 0.2028 - val_accuracy: 0.9302\n",
      "Epoch 82/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2081 - accuracy: 0.9266 - val_loss: 0.2017 - val_accuracy: 0.9306\n",
      "Epoch 83/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2077 - accuracy: 0.9268 - val_loss: 0.2022 - val_accuracy: 0.9291\n",
      "Epoch 84/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2074 - accuracy: 0.9267 - val_loss: 0.2014 - val_accuracy: 0.9306\n",
      "Epoch 85/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2069 - accuracy: 0.9272 - val_loss: 0.2009 - val_accuracy: 0.9313\n",
      "Epoch 86/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2066 - accuracy: 0.9269 - val_loss: 0.2008 - val_accuracy: 0.9310\n",
      "Epoch 87/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2063 - accuracy: 0.9270 - val_loss: 0.2006 - val_accuracy: 0.9310\n",
      "Epoch 88/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2060 - accuracy: 0.9272 - val_loss: 0.1996 - val_accuracy: 0.9328\n",
      "Epoch 89/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2057 - accuracy: 0.9272 - val_loss: 0.1996 - val_accuracy: 0.9310\n",
      "Epoch 90/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2053 - accuracy: 0.9276 - val_loss: 0.1996 - val_accuracy: 0.9317\n",
      "Epoch 91/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2050 - accuracy: 0.9271 - val_loss: 0.1997 - val_accuracy: 0.9321\n",
      "Epoch 92/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2047 - accuracy: 0.9278 - val_loss: 0.1984 - val_accuracy: 0.9325\n",
      "Epoch 93/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2044 - accuracy: 0.9279 - val_loss: 0.1981 - val_accuracy: 0.9328\n",
      "Epoch 94/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2042 - accuracy: 0.9279 - val_loss: 0.1985 - val_accuracy: 0.9328\n",
      "Epoch 95/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2038 - accuracy: 0.9277 - val_loss: 0.1983 - val_accuracy: 0.9313\n",
      "Epoch 96/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2036 - accuracy: 0.9280 - val_loss: 0.1980 - val_accuracy: 0.9325\n",
      "Epoch 97/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2033 - accuracy: 0.9281 - val_loss: 0.1976 - val_accuracy: 0.9321\n",
      "Epoch 98/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.2030 - accuracy: 0.9282 - val_loss: 0.1970 - val_accuracy: 0.9328\n",
      "Epoch 99/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2028 - accuracy: 0.9281 - val_loss: 0.1973 - val_accuracy: 0.9317\n",
      "Epoch 100/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2025 - accuracy: 0.9282 - val_loss: 0.1970 - val_accuracy: 0.9325\n",
      "Epoch 101/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2023 - accuracy: 0.9285 - val_loss: 0.1965 - val_accuracy: 0.9328\n",
      "Epoch 102/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2020 - accuracy: 0.9289 - val_loss: 0.1968 - val_accuracy: 0.9321\n",
      "Epoch 103/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2018 - accuracy: 0.9285 - val_loss: 0.1964 - val_accuracy: 0.9321\n",
      "Epoch 104/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2016 - accuracy: 0.9290 - val_loss: 0.1960 - val_accuracy: 0.9321\n",
      "Epoch 105/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2013 - accuracy: 0.9286 - val_loss: 0.1956 - val_accuracy: 0.9340\n",
      "Epoch 106/150\n",
      "189/189 [==============================] - 3s 15ms/step - loss: 0.2011 - accuracy: 0.9291 - val_loss: 0.1952 - val_accuracy: 0.9336\n",
      "Epoch 107/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2009 - accuracy: 0.9290 - val_loss: 0.1956 - val_accuracy: 0.9328\n",
      "Epoch 108/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2007 - accuracy: 0.9289 - val_loss: 0.1952 - val_accuracy: 0.9328\n",
      "Epoch 109/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2005 - accuracy: 0.9290 - val_loss: 0.1950 - val_accuracy: 0.9332\n",
      "Epoch 110/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.2003 - accuracy: 0.9293 - val_loss: 0.1951 - val_accuracy: 0.9332\n",
      "Epoch 111/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.2001 - accuracy: 0.9292 - val_loss: 0.1946 - val_accuracy: 0.9328\n",
      "Epoch 112/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.1999 - accuracy: 0.9292 - val_loss: 0.1945 - val_accuracy: 0.9336\n",
      "Epoch 113/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1997 - accuracy: 0.9292 - val_loss: 0.1940 - val_accuracy: 0.9336\n",
      "Epoch 114/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1994 - accuracy: 0.9293 - val_loss: 0.1940 - val_accuracy: 0.9332\n",
      "Epoch 115/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.1992 - accuracy: 0.9294 - val_loss: 0.1937 - val_accuracy: 0.9343\n",
      "Epoch 116/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1991 - accuracy: 0.9295 - val_loss: 0.1937 - val_accuracy: 0.9351\n",
      "Epoch 117/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1990 - accuracy: 0.9297 - val_loss: 0.1942 - val_accuracy: 0.9332\n",
      "Epoch 118/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.1988 - accuracy: 0.9298 - val_loss: 0.1934 - val_accuracy: 0.9336\n",
      "Epoch 119/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.1986 - accuracy: 0.9296 - val_loss: 0.1934 - val_accuracy: 0.9340\n",
      "Epoch 120/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1984 - accuracy: 0.9295 - val_loss: 0.1932 - val_accuracy: 0.9336\n",
      "Epoch 121/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1983 - accuracy: 0.9299 - val_loss: 0.1932 - val_accuracy: 0.9343\n",
      "Epoch 122/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.1980 - accuracy: 0.9298 - val_loss: 0.1929 - val_accuracy: 0.9340\n",
      "Epoch 123/150\n",
      "189/189 [==============================] - 3s 15ms/step - loss: 0.1979 - accuracy: 0.9298 - val_loss: 0.1925 - val_accuracy: 0.9340\n",
      "Epoch 124/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.1977 - accuracy: 0.9302 - val_loss: 0.1924 - val_accuracy: 0.9340\n",
      "Epoch 125/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1976 - accuracy: 0.9300 - val_loss: 0.1927 - val_accuracy: 0.9336\n",
      "Epoch 126/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1974 - accuracy: 0.9298 - val_loss: 0.1923 - val_accuracy: 0.9351\n",
      "Epoch 127/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.1973 - accuracy: 0.9304 - val_loss: 0.1919 - val_accuracy: 0.9347\n",
      "Epoch 128/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1971 - accuracy: 0.9299 - val_loss: 0.1919 - val_accuracy: 0.9351\n",
      "Epoch 129/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1970 - accuracy: 0.9303 - val_loss: 0.1924 - val_accuracy: 0.9343\n",
      "Epoch 130/150\n",
      "189/189 [==============================] - 3s 15ms/step - loss: 0.1968 - accuracy: 0.9301 - val_loss: 0.1917 - val_accuracy: 0.9340\n",
      "Epoch 131/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1967 - accuracy: 0.9301 - val_loss: 0.1916 - val_accuracy: 0.9332\n",
      "Epoch 132/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.1965 - accuracy: 0.9304 - val_loss: 0.1924 - val_accuracy: 0.9321\n",
      "Epoch 133/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1964 - accuracy: 0.9303 - val_loss: 0.1917 - val_accuracy: 0.9325\n",
      "Epoch 134/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.1962 - accuracy: 0.9305 - val_loss: 0.1911 - val_accuracy: 0.9340\n",
      "Epoch 135/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1961 - accuracy: 0.9308 - val_loss: 0.1914 - val_accuracy: 0.9336\n",
      "Epoch 136/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1960 - accuracy: 0.9305 - val_loss: 0.1915 - val_accuracy: 0.9321\n",
      "Epoch 137/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1957 - accuracy: 0.9305 - val_loss: 0.1905 - val_accuracy: 0.9343\n",
      "Epoch 138/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1957 - accuracy: 0.9307 - val_loss: 0.1906 - val_accuracy: 0.9347\n",
      "Epoch 139/150\n",
      "189/189 [==============================] - 3s 16ms/step - loss: 0.1955 - accuracy: 0.9308 - val_loss: 0.1911 - val_accuracy: 0.9336\n",
      "Epoch 140/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.1954 - accuracy: 0.9306 - val_loss: 0.1908 - val_accuracy: 0.9340\n",
      "Epoch 141/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.1953 - accuracy: 0.9307 - val_loss: 0.1904 - val_accuracy: 0.9343\n",
      "Epoch 142/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1951 - accuracy: 0.9307 - val_loss: 0.1901 - val_accuracy: 0.9340\n",
      "Epoch 143/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.1950 - accuracy: 0.9303 - val_loss: 0.1902 - val_accuracy: 0.9343\n",
      "Epoch 144/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.1949 - accuracy: 0.9303 - val_loss: 0.1900 - val_accuracy: 0.9343\n",
      "Epoch 145/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.1948 - accuracy: 0.9307 - val_loss: 0.1898 - val_accuracy: 0.9347\n",
      "Epoch 146/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1947 - accuracy: 0.9313 - val_loss: 0.1897 - val_accuracy: 0.9351\n",
      "Epoch 147/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1946 - accuracy: 0.9314 - val_loss: 0.1899 - val_accuracy: 0.9343\n",
      "Epoch 148/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1944 - accuracy: 0.9312 - val_loss: 0.1903 - val_accuracy: 0.9325\n",
      "Epoch 149/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1944 - accuracy: 0.9308 - val_loss: 0.1893 - val_accuracy: 0.9343\n",
      "Epoch 150/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1942 - accuracy: 0.9309 - val_loss: 0.1898 - val_accuracy: 0.9340\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.1913 - accuracy: 0.9289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████                                                                    | 2/10 [10:09<40:39, 304.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1912831962108612, 0.9288636445999146]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/40000 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|██▎                                                                          | 1192/40000 [00:00<00:03, 11918.14it/s]\u001b[A\n",
      "  6%|████▌                                                                        | 2384/40000 [00:00<00:03, 11860.48it/s]\u001b[A\n",
      "  9%|██████▉                                                                      | 3573/40000 [00:00<00:03, 11871.28it/s]\u001b[A\n",
      " 12%|█████████▏                                                                   | 4761/40000 [00:00<00:02, 11856.52it/s]\u001b[A\n",
      " 15%|███████████▍                                                                 | 5947/40000 [00:00<00:02, 11857.31it/s]\u001b[A\n",
      " 18%|█████████████▋                                                               | 7134/40000 [00:00<00:02, 11860.40it/s]\u001b[A\n",
      " 21%|████████████████                                                             | 8323/40000 [00:00<00:02, 11867.97it/s]\u001b[A\n",
      " 24%|██████████████████▎                                                          | 9510/40000 [00:00<00:02, 11868.17it/s]\u001b[A\n",
      " 27%|████████████████████▎                                                       | 10698/40000 [00:00<00:02, 11869.66it/s]\u001b[A\n",
      " 30%|██████████████████████▌                                                     | 11885/40000 [00:01<00:02, 11866.66it/s]\u001b[A\n",
      " 33%|████████████████████████▊                                                   | 13072/40000 [00:01<00:02, 11863.72it/s]\u001b[A\n",
      " 36%|███████████████████████████                                                 | 14260/40000 [00:01<00:02, 11865.61it/s]\u001b[A\n",
      " 39%|█████████████████████████████▎                                              | 15447/40000 [00:01<00:02, 11851.61it/s]\u001b[A\n",
      " 42%|███████████████████████████████▌                                            | 16633/40000 [00:01<00:01, 11844.36it/s]\u001b[A\n",
      " 45%|█████████████████████████████████▊                                          | 17818/40000 [00:01<00:01, 11840.71it/s]\u001b[A\n",
      " 48%|████████████████████████████████████                                        | 19004/40000 [00:01<00:01, 11845.76it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████▎                                     | 20192/40000 [00:01<00:01, 11854.56it/s]\u001b[A\n",
      " 53%|████████████████████████████████████████▌                                   | 21378/40000 [00:01<00:01, 11832.85it/s]\u001b[A\n",
      " 56%|██████████████████████████████████████████▊                                 | 22562/40000 [00:01<00:01, 11809.69it/s]\u001b[A\n",
      " 59%|█████████████████████████████████████████████                               | 23743/40000 [00:02<00:01, 11801.55it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████▎                            | 24924/40000 [00:02<00:01, 11593.44it/s]\u001b[A\n",
      " 65%|█████████████████████████████████████████████████▌                          | 26099/40000 [00:02<00:01, 11637.93it/s]\u001b[A\n",
      " 68%|███████████████████████████████████████████████████▊                        | 27280/40000 [00:02<00:01, 11687.94it/s]\u001b[A\n",
      " 71%|██████████████████████████████████████████████████████                      | 28461/40000 [00:02<00:00, 11721.84it/s]\u001b[A\n",
      " 74%|████████████████████████████████████████████████████████▎                   | 29643/40000 [00:02<00:00, 11748.94it/s]\u001b[A\n",
      " 77%|██████████████████████████████████████████████████████████▌                 | 30824/40000 [00:02<00:00, 11765.96it/s]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████████▊               | 32001/40000 [00:02<00:00, 11076.50it/s]\u001b[A\n",
      " 83%|██████████████████████████████████████████████████████████████▉             | 33117/40000 [00:02<00:00, 10849.31it/s]\u001b[A\n",
      " 86%|████████████████████████████████████████████████████████████████▉           | 34208/40000 [00:02<00:00, 10741.82it/s]\u001b[A\n",
      " 88%|███████████████████████████████████████████████████████████████████         | 35287/40000 [00:03<00:00, 10561.65it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████▏      | 36432/40000 [00:03<00:00, 10816.79it/s]\u001b[A\n",
      " 94%|███████████████████████████████████████████████████████████████████████▍    | 37593/40000 [00:03<00:00, 11046.18it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████████████████████████████████████▋  | 38770/40000 [00:03<00:00, 11256.56it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:03<00:00, 11572.22it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "(40000, 16, 256, 1)\n",
      "(40000, 10)\n",
      "Epoch 1/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 1.8960 - accuracy: 0.2927 - val_loss: 1.5691 - val_accuracy: 0.3340\n",
      "Epoch 2/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 1.3182 - accuracy: 0.5116 - val_loss: 1.1026 - val_accuracy: 0.6668\n",
      "Epoch 3/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.9595 - accuracy: 0.7245 - val_loss: 0.8295 - val_accuracy: 0.7873\n",
      "Epoch 4/150\n",
      "189/189 [==============================] - 3s 13ms/step - loss: 0.7514 - accuracy: 0.8235 - val_loss: 0.6712 - val_accuracy: 0.8526\n",
      "Epoch 5/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.6247 - accuracy: 0.8657 - val_loss: 0.5703 - val_accuracy: 0.8799\n",
      "Epoch 6/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.5411 - accuracy: 0.8826 - val_loss: 0.5014 - val_accuracy: 0.8959\n",
      "Epoch 7/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.4822 - accuracy: 0.8922 - val_loss: 0.4512 - val_accuracy: 0.9022\n",
      "Epoch 8/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.4383 - accuracy: 0.8993 - val_loss: 0.4131 - val_accuracy: 0.9090\n",
      "Epoch 9/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.4042 - accuracy: 0.9041 - val_loss: 0.3831 - val_accuracy: 0.9127\n",
      "Epoch 10/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.3768 - accuracy: 0.9074 - val_loss: 0.3586 - val_accuracy: 0.9142\n",
      "Epoch 11/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.3542 - accuracy: 0.9101 - val_loss: 0.3381 - val_accuracy: 0.9160\n",
      "Epoch 12/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.3352 - accuracy: 0.9124 - val_loss: 0.3210 - val_accuracy: 0.9205\n",
      "Epoch 13/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.3190 - accuracy: 0.9163 - val_loss: 0.3061 - val_accuracy: 0.9216\n",
      "Epoch 14/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.3048 - accuracy: 0.9176 - val_loss: 0.2932 - val_accuracy: 0.9228\n",
      "Epoch 15/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2925 - accuracy: 0.9191 - val_loss: 0.2819 - val_accuracy: 0.9246\n",
      "Epoch 16/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.2816 - accuracy: 0.9209 - val_loss: 0.2719 - val_accuracy: 0.9257\n",
      "Epoch 17/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2718 - accuracy: 0.9218 - val_loss: 0.2630 - val_accuracy: 0.9257\n",
      "Epoch 18/150\n",
      "189/189 [==============================] - 3s 16ms/step - loss: 0.2630 - accuracy: 0.9225 - val_loss: 0.2551 - val_accuracy: 0.9257\n",
      "Epoch 19/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2551 - accuracy: 0.9237 - val_loss: 0.2479 - val_accuracy: 0.9261\n",
      "Epoch 20/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.2479 - accuracy: 0.9246 - val_loss: 0.2413 - val_accuracy: 0.9269\n",
      "Epoch 21/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.2414 - accuracy: 0.9259 - val_loss: 0.2355 - val_accuracy: 0.9299\n",
      "Epoch 22/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.2354 - accuracy: 0.9267 - val_loss: 0.2301 - val_accuracy: 0.9299\n",
      "Epoch 23/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.2300 - accuracy: 0.9273 - val_loss: 0.2252 - val_accuracy: 0.9306\n",
      "Epoch 24/150\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 0.2249 - accuracy: 0.9280 - val_loss: 0.2206 - val_accuracy: 0.9313\n",
      "Epoch 25/150\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 0.2203 - accuracy: 0.9288 - val_loss: 0.2166 - val_accuracy: 0.9310\n",
      "Epoch 26/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2160 - accuracy: 0.9291 - val_loss: 0.2129 - val_accuracy: 0.9313\n",
      "Epoch 27/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2120 - accuracy: 0.9301 - val_loss: 0.2095 - val_accuracy: 0.9313\n",
      "Epoch 28/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2083 - accuracy: 0.9309 - val_loss: 0.2060 - val_accuracy: 0.9313\n",
      "Epoch 29/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.2049 - accuracy: 0.9311 - val_loss: 0.2030 - val_accuracy: 0.9313\n",
      "Epoch 30/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.2017 - accuracy: 0.9321 - val_loss: 0.2003 - val_accuracy: 0.9325\n",
      "Epoch 31/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1987 - accuracy: 0.9332 - val_loss: 0.1976 - val_accuracy: 0.9328\n",
      "Epoch 32/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1959 - accuracy: 0.9337 - val_loss: 0.1950 - val_accuracy: 0.9328\n",
      "Epoch 33/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1932 - accuracy: 0.9343 - val_loss: 0.1928 - val_accuracy: 0.9336\n",
      "Epoch 34/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1908 - accuracy: 0.9344 - val_loss: 0.1909 - val_accuracy: 0.9347\n",
      "Epoch 35/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.1886 - accuracy: 0.9347 - val_loss: 0.1888 - val_accuracy: 0.9347\n",
      "Epoch 36/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1864 - accuracy: 0.9351 - val_loss: 0.1871 - val_accuracy: 0.9354\n",
      "Epoch 37/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.1844 - accuracy: 0.9357 - val_loss: 0.1851 - val_accuracy: 0.9354\n",
      "Epoch 38/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1825 - accuracy: 0.9360 - val_loss: 0.1837 - val_accuracy: 0.9354\n",
      "Epoch 39/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1807 - accuracy: 0.9362 - val_loss: 0.1824 - val_accuracy: 0.9366\n",
      "Epoch 40/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.1791 - accuracy: 0.9365 - val_loss: 0.1805 - val_accuracy: 0.9351\n",
      "Epoch 41/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1775 - accuracy: 0.9369 - val_loss: 0.1793 - val_accuracy: 0.9362\n",
      "Epoch 42/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1760 - accuracy: 0.9371 - val_loss: 0.1779 - val_accuracy: 0.9362\n",
      "Epoch 43/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1746 - accuracy: 0.9371 - val_loss: 0.1769 - val_accuracy: 0.9366\n",
      "Epoch 44/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1733 - accuracy: 0.9370 - val_loss: 0.1757 - val_accuracy: 0.9369\n",
      "Epoch 45/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.1721 - accuracy: 0.9373 - val_loss: 0.1747 - val_accuracy: 0.9373\n",
      "Epoch 46/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1708 - accuracy: 0.9379 - val_loss: 0.1738 - val_accuracy: 0.9388\n",
      "Epoch 47/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1698 - accuracy: 0.9376 - val_loss: 0.1726 - val_accuracy: 0.9377\n",
      "Epoch 48/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1687 - accuracy: 0.9379 - val_loss: 0.1717 - val_accuracy: 0.9381\n",
      "Epoch 49/150\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 0.1677 - accuracy: 0.9377 - val_loss: 0.1709 - val_accuracy: 0.9388\n",
      "Epoch 50/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1667 - accuracy: 0.9381 - val_loss: 0.1700 - val_accuracy: 0.9388\n",
      "Epoch 51/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1658 - accuracy: 0.9385 - val_loss: 0.1694 - val_accuracy: 0.9384\n",
      "Epoch 52/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1649 - accuracy: 0.9383 - val_loss: 0.1689 - val_accuracy: 0.9381\n",
      "Epoch 53/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.1641 - accuracy: 0.9381 - val_loss: 0.1679 - val_accuracy: 0.9384\n",
      "Epoch 54/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1633 - accuracy: 0.9386 - val_loss: 0.1672 - val_accuracy: 0.9381\n",
      "Epoch 55/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1625 - accuracy: 0.9379 - val_loss: 0.1667 - val_accuracy: 0.9381\n",
      "Epoch 56/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1619 - accuracy: 0.9384 - val_loss: 0.1660 - val_accuracy: 0.9377\n",
      "Epoch 57/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.1612 - accuracy: 0.9383 - val_loss: 0.1655 - val_accuracy: 0.9381\n",
      "Epoch 58/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.1605 - accuracy: 0.9383 - val_loss: 0.1648 - val_accuracy: 0.9381\n",
      "Epoch 59/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1598 - accuracy: 0.9382 - val_loss: 0.1643 - val_accuracy: 0.9381\n",
      "Epoch 60/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1592 - accuracy: 0.9380 - val_loss: 0.1640 - val_accuracy: 0.9377\n",
      "Epoch 61/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1586 - accuracy: 0.9379 - val_loss: 0.1634 - val_accuracy: 0.9384\n",
      "Epoch 62/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1581 - accuracy: 0.9381 - val_loss: 0.1629 - val_accuracy: 0.9377\n",
      "Epoch 63/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1575 - accuracy: 0.9383 - val_loss: 0.1622 - val_accuracy: 0.9384\n",
      "Epoch 64/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.1570 - accuracy: 0.9378 - val_loss: 0.1618 - val_accuracy: 0.9381\n",
      "Epoch 65/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1565 - accuracy: 0.9379 - val_loss: 0.1614 - val_accuracy: 0.9381\n",
      "Epoch 66/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1560 - accuracy: 0.9379 - val_loss: 0.1611 - val_accuracy: 0.9373\n",
      "Epoch 67/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1556 - accuracy: 0.9376 - val_loss: 0.1606 - val_accuracy: 0.9377\n",
      "Epoch 68/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1551 - accuracy: 0.9379 - val_loss: 0.1603 - val_accuracy: 0.9373\n",
      "Epoch 69/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1546 - accuracy: 0.9378 - val_loss: 0.1603 - val_accuracy: 0.9366\n",
      "Epoch 70/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1542 - accuracy: 0.9376 - val_loss: 0.1596 - val_accuracy: 0.9369\n",
      "Epoch 71/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1538 - accuracy: 0.9376 - val_loss: 0.1591 - val_accuracy: 0.9373\n",
      "Epoch 72/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1534 - accuracy: 0.9376 - val_loss: 0.1588 - val_accuracy: 0.9373\n",
      "Epoch 73/150\n",
      "189/189 [==============================] - 1s 8ms/step - loss: 0.1530 - accuracy: 0.9372 - val_loss: 0.1590 - val_accuracy: 0.9362\n",
      "Epoch 74/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1526 - accuracy: 0.9371 - val_loss: 0.1587 - val_accuracy: 0.9362\n",
      "Epoch 75/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1523 - accuracy: 0.9374 - val_loss: 0.1581 - val_accuracy: 0.9358\n",
      "Epoch 76/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1519 - accuracy: 0.9372 - val_loss: 0.1574 - val_accuracy: 0.9377\n",
      "Epoch 77/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.1516 - accuracy: 0.9369 - val_loss: 0.1574 - val_accuracy: 0.9358\n",
      "Epoch 78/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1513 - accuracy: 0.9366 - val_loss: 0.1570 - val_accuracy: 0.9362\n",
      "Epoch 79/150\n",
      "189/189 [==============================] - 1s 6ms/step - loss: 0.1509 - accuracy: 0.9372 - val_loss: 0.1568 - val_accuracy: 0.9366\n",
      "Epoch 80/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1506 - accuracy: 0.9368 - val_loss: 0.1566 - val_accuracy: 0.9358\n",
      "Epoch 81/150\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 0.1503 - accuracy: 0.9370 - val_loss: 0.1561 - val_accuracy: 0.9362\n",
      "Epoch 82/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.1499 - accuracy: 0.9372 - val_loss: 0.1562 - val_accuracy: 0.9366\n",
      "Epoch 83/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1497 - accuracy: 0.9371 - val_loss: 0.1556 - val_accuracy: 0.9366\n",
      "Epoch 84/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1493 - accuracy: 0.9367 - val_loss: 0.1558 - val_accuracy: 0.9366\n",
      "Epoch 85/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1491 - accuracy: 0.9364 - val_loss: 0.1550 - val_accuracy: 0.9369\n",
      "Epoch 86/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1488 - accuracy: 0.9364 - val_loss: 0.1549 - val_accuracy: 0.9369\n",
      "Epoch 87/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.1485 - accuracy: 0.9371 - val_loss: 0.1544 - val_accuracy: 0.9366\n",
      "Epoch 88/150\n",
      "189/189 [==============================] - 2s 9ms/step - loss: 0.1483 - accuracy: 0.9368 - val_loss: 0.1544 - val_accuracy: 0.9369\n",
      "Epoch 89/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.1480 - accuracy: 0.9364 - val_loss: 0.1542 - val_accuracy: 0.9373\n",
      "Epoch 90/150\n",
      "189/189 [==============================] - 2s 10ms/step - loss: 0.1477 - accuracy: 0.9365 - val_loss: 0.1543 - val_accuracy: 0.9366\n",
      "Epoch 91/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.1475 - accuracy: 0.9363 - val_loss: 0.1537 - val_accuracy: 0.9369\n",
      "Epoch 92/150\n",
      "189/189 [==============================] - 2s 8ms/step - loss: 0.1472 - accuracy: 0.9360 - val_loss: 0.1536 - val_accuracy: 0.9373\n",
      "Epoch 93/150\n",
      "189/189 [==============================] - 3s 14ms/step - loss: 0.1470 - accuracy: 0.9370 - val_loss: 0.1536 - val_accuracy: 0.9366\n",
      "Epoch 94/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1468 - accuracy: 0.9355 - val_loss: 0.1530 - val_accuracy: 0.9381\n",
      "Epoch 95/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.1465 - accuracy: 0.9365 - val_loss: 0.1529 - val_accuracy: 0.9373\n",
      "Epoch 96/150\n",
      "189/189 [==============================] - 1s 7ms/step - loss: 0.1463 - accuracy: 0.9364 - val_loss: 0.1527 - val_accuracy: 0.9373\n",
      "Epoch 97/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.1460 - accuracy: 0.9360 - val_loss: 0.1524 - val_accuracy: 0.9373\n",
      "Epoch 98/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.1458 - accuracy: 0.9364 - val_loss: 0.1524 - val_accuracy: 0.9373\n",
      "Epoch 99/150\n",
      "189/189 [==============================] - 3s 15ms/step - loss: 0.1456 - accuracy: 0.9355 - val_loss: 0.1520 - val_accuracy: 0.9377\n",
      "Epoch 100/150\n",
      "189/189 [==============================] - 1s 5ms/step - loss: 0.1455 - accuracy: 0.9363 - val_loss: 0.1519 - val_accuracy: 0.9377\n",
      "Epoch 101/150\n",
      "116/189 [=================>............] - ETA: 0s - loss: 0.1437 - accuracy: 0.9365"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "total_scores = []\n",
    "for i in tqdm(range(10)):\n",
    "    idealized_signals, labels = idealized(num = 10_000)\n",
    "    input_data = np.expand_dims(normalize_data(idealized_signals), axis = -1)\n",
    "    print(input_data[0,:,:].max(), input_data[0,:,:].min())\n",
    "    splits = input_data.shape[0]//1_000\n",
    "    X = []\n",
    "    print(input_data.shape)\n",
    "    for i in range(splits):\n",
    "        tensor = tf.convert_to_tensor(input_data[i*1_000:(i+1)*1_000, :,:,:], dtype=tf.float32)\n",
    "        X.append(autoencoder.encoder(tensor)[0])\n",
    "    X = np.vstack(X)\n",
    "    print(X.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "    linear = keras.Sequential(name=\"my_sequential\")\n",
    "    linear.add(layers.Dense(4, activation=\"sigmoid\", name=\"layer2\"))\n",
    "\n",
    "    linear.compile(loss='categorical_crossentropy', \n",
    "                   optimizer=keras.optimizers.Adam(learning_rate = 1e-3), metrics=['accuracy'])\n",
    "    linear.fit(X_train, y_train, epochs=150, batch_size=128, validation_split = 0.1, shuffle=True)\n",
    "    results = linear.evaluate(X_test, y_test)\n",
    "    print(results)\n",
    "    total_scores.append(results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd8485-e07e-4a7e-adaf-6709461d3e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.mean(total_scores))\n",
    "print(np.std(total_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
